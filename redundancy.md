# Definition

1.  In information theory: of a source, the amount by which the
    logarithm of the number of symbols available at the source exceeds
    the average information content per symbol of the source. The term
    redundancy has been used loosely in other senses. For example, a
    source whose output is normally transmitted over a given channel has
    been called redundant, if the channel utilization index is less than
    unity. 2. The existence of more than one means for accomplishing a
    given task, where all means must fail before there is an overall
    failure to the system. Parallel redundancy applies to systems where
    both means are working at the same time to accomplish the task, and
    either of the system is capable of handling the job itself in case
    of failure of the other system. Standby redundancy applies to a
    system where there is an alternative means of accomplishing the task
    that is switched in by a malfunction sensing device when the primary
    system fails.
